{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ozmel1/transformers/blob/main/DeepSeek_R1_Distill_Qwen_1_5B_MATH_SFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f541c4a5",
      "metadata": {
        "id": "f541c4a5"
      },
      "source": [
        "### Overview\n",
        "* This notebook fine-tunes a 8-bit quantized `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` on a small subset of MATH dataset on a single Colab T4 GPU.\n",
        "* Under limited GPU RAM (15.0GB), we applied LoRA and ZeRO stage 3 for CPU swapping on model weights, optimizer states and activations.\n",
        "* Libraries used: `transformers`, `deepspeed`, `bitsandbytes`, `peft`, `datasets`, `evaluate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5908a9b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5908a9b2",
        "outputId": "c3935486-0f1b-4dda-f9d6-863ec48a0430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Running on {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5810cffa",
      "metadata": {
        "id": "5810cffa"
      },
      "source": [
        "* Installing packages on Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b2a51e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b2a51e9",
        "outputId": "6eef9ea2-27a1-4e4a-cf2f-002993bc8561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[deepspeed] in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[deepspeed]) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers[deepspeed]) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[deepspeed]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[deepspeed]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[deepspeed]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[deepspeed]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[deepspeed]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[deepspeed]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[deepspeed]) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[deepspeed]) (4.67.1)\n",
            "Requirement already satisfied: deepspeed>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from transformers[deepspeed]) (0.16.7)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[deepspeed]) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[deepspeed]) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[deepspeed]) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed>=0.9.3->transformers[deepspeed]) (0.8.1)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.11/dist-packages (from deepspeed>=0.9.3->transformers[deepspeed]) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed>=0.9.3->transformers[deepspeed]) (1.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from deepspeed>=0.9.3->transformers[deepspeed]) (1.11.1.4)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed>=0.9.3->transformers[deepspeed]) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed>=0.9.3->transformers[deepspeed]) (2.11.4)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed>=0.9.3->transformers[deepspeed]) (12.570.86)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[deepspeed]) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[deepspeed]) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[deepspeed]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[deepspeed]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[deepspeed]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[deepspeed]) (2025.4.26)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed>=0.9.3->transformers[deepspeed]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed>=0.9.3->transformers[deepspeed]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed>=0.9.3->transformers[deepspeed]) (0.4.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0->transformers[deepspeed]) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers[deepspeed]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97d0ebfa",
      "metadata": {
        "id": "97d0ebfa"
      },
      "source": [
        " * Reload Colab after the following, for `deepspeed` and `bitsandbytes` to work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f3bc2f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f3bc2f2",
        "outputId": "cd79bc00-276a-4f19-b981-ee3d56a9e8f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51ef31d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51ef31d0",
        "outputId": "0d999480-1c4c-48e1-afe8-12d4825195eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f03195",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72f03195",
        "outputId": "aa36bfd2-895a-4f34-a574-3aa3f6c2b810"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.0.3.tar.gz (466 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/466.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.7/466.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.3-cp311-cp311-linux_x86_64.whl size=4438165 sha256=d5a865eded78fac9e7bc48dd9f83b7769fd892031b6d3c962ab8c5c2555c29f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/56/17/bf6ba37aa971a191a8b9eaa188bf5ec855b8911c1c56fb1f84\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.3\n"
          ]
        }
      ],
      "source": [
        "pip install mpi4py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a5f45c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a5f45c5",
        "outputId": "077171a3-0c86-42c1-cbc7-2cd1340a25da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ],
      "source": [
        "pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baeedb88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baeedb88",
        "outputId": "d0e9e295-b309-48a5-e094-77a9e94519a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/hendrycks/math.git\n",
            "  Cloning https://github.com/hendrycks/math.git to /tmp/pip-req-build-motkvtmf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/hendrycks/math.git /tmp/pip-req-build-motkvtmf\n",
            "  Resolved https://github.com/hendrycks/math.git to commit 357963a7f5501a6c1708cf3f3fb0cdf525642761\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: math_equivalence\n",
            "  Building wheel for math_equivalence (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for math_equivalence: filename=math_equivalence-0.0.0-py3-none-any.whl size=3501 sha256=be5f4eb7659bc5f49176b6f3c99883c6d18cc3ec7282f228a8a9575b02af7dbf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mym0mn5h/wheels/b7/16/f0/4a69d4d9b720086e22842cbd2d896b66298e6424b8f289f37c\n",
            "Successfully built math_equivalence\n",
            "Installing collected packages: math_equivalence\n",
            "Successfully installed math_equivalence-0.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/hendrycks/math.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dedd389a",
      "metadata": {
        "id": "dedd389a"
      },
      "source": [
        "### Load models and apply quantization\n",
        "*   `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B`: 1.78B, stored in BF16\n",
        "*   When using Google T4 GPU, which has 14.74GB space actually available.\n",
        "    *   We need to apply 8-bit quantization and LoRA to have more spaces available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efe1591a",
      "metadata": {
        "id": "efe1591a"
      },
      "outputs": [],
      "source": [
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91de586a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "7c7fccf8ead14fc88ed430f022da20d2",
            "a80e32f5b7ca45adaf699c9e5e425813",
            "536f44410eaf43969ce8b868c6e18775",
            "60e274a5c78547609c1d6940175cd9a3",
            "ee104fb8b64f4b10a7c2bbcf4e092782",
            "dedc96d3efe54b9b8c9954dd6e7dae2a",
            "5a65b1e6b36d4a72bcdf0a324174e8d1",
            "f872b059565f40a5877ffc214ab43135",
            "2a2fbaba7d354c6c9e50d46d3162477b",
            "8cde1b3e0355476281a4629bb35f315c",
            "d2769f7befe74a0895ea5a32b0278938",
            "55f36e867dd34c9eaa47b9ef48410079",
            "a0abd40e03a24c6b8836b6a290cb9a20",
            "c45a06776b374abaade0c6ef8d327246",
            "cf25d98651f646c997d559d4a1ebe990",
            "faab3841fce44d4e9dba7c87c8185130",
            "cd4894a153ef4627b5bb37066a9149da",
            "ef0f221693a64f79a05b166d99455bd8",
            "8b5f70237ad54e739988e0216a8d2604",
            "63e59d1ddfb347a5837e186ea6bd6960",
            "069ef056fe5143009dbbbf6974eb4700",
            "5783b15a427f40888c7d3ac305b3bd7b"
          ]
        },
        "id": "91de586a",
        "outputId": "777631b0-d999-4895-9d75-8afe07671b99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c7fccf8ead14fc88ed430f022da20d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55f36e867dd34c9eaa47b9ef48410079",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_has_fp16_weight=False # Not keeping the FP16 copy\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c11c181",
      "metadata": {
        "id": "4c11c181"
      },
      "source": [
        "*   The following loads the model and quantize it.\n",
        "\n",
        "*   Size of the model:\n",
        "    *   Each parameter: 8 bit = 1 byte\n",
        "    *   All parameters: 1.78B * 1 byte = 1.78GB\n",
        "\n",
        "*   We can see 1.8GB GPU memory is occupied, which stands for the quantized model weights.\n",
        "*   After the loading is finished, we have 2.3GB in GPU.\n",
        "    *   Additional spaces can be occupied by buffers, or empty for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7281315",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148,
          "referenced_widgets": [
            "2946406cb9984b3bb47677d696dc2e5f",
            "54bc79facfd8444ca349cea584ec8611",
            "e8d79b2c2e4c483c8c57cf3bd5324773",
            "2155f8d51d6248479fe3c92c19fe2adb",
            "aad351a9b7374507b82ccaa73c5aebe9",
            "76d73964c0ed4f7e91d2215d83702491",
            "3499221da24a439e98c0f74116b24bd5",
            "2fa1a4f6c4c14b1bbfa83eb0380372b6",
            "8f0967da3cbe4fcd98988fe7dc448312",
            "cd79c15448bd48ceb5883de041e734a7",
            "d96bab6cf11947dcbccccffdeddeab0b",
            "e1ba9c878f2d45ca9fc62171a60f8cbf",
            "2577ac8af9594e58b277aa090b4bf2be",
            "0e4ecb3b3e5747849748aeb37edccd85",
            "be86d3528ca34ed08c672bac49f6d98e",
            "fd4c2ffacec647ec994d42d4069ffe58",
            "f501983d7d4a4435936191d8f7b04a7f",
            "a0b5ab8b53d5445f85de80e5faeaf0f9",
            "93f2c092582a4d49a4f752eb39d7eaab",
            "d1ceb69b5bed4b5d8bb549c765bfd240",
            "92dcf10f2a4242c3b6775fe8d16dc9e5",
            "440fda80734443c8a6a54838bf7ffcf0",
            "5a322dec3c8445bba30fb6bac71a8897",
            "8054850f8f6841ec9551126c8858dad1",
            "44d07be3fe2248beb8c688571b63d248",
            "ce9accccf12140e7ad25c63178bd12ed",
            "7f53f455a4744e969de4f2f2c9bb6ec6",
            "bf41a8a835a543eaba98cd6b363bd9c8",
            "8087adacd03b4f89813f7b5a1ed5bacc",
            "c2adb2a9bb6f4c88bad60e8fdbe8a105",
            "3c919787c2df4372b4614b769eeab8a5",
            "c8c10309c2a242f192da11bc9c08c9a3",
            "ca9cfefb21d84e49b1e6ef8cf8acbf5b"
          ]
        },
        "id": "a7281315",
        "outputId": "aab6791a-8e72-4039-e025-e1afec1c2ae2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2946406cb9984b3bb47677d696dc2e5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-11 23:40:46,214] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1ba9c878f2d45ca9fc62171a60f8cbf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a322dec3c8445bba30fb6bac71a8897",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8b9971e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8b9971e",
        "outputId": "8cbafca9-0000-4809-c9db-b4ab45776f41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Allocated space in GPU by PyTorch: 2.27670784 GB\n",
            "Reserved space in GPU by PyTorch: 2.363490304 GB\n"
          ]
        }
      ],
      "source": [
        "def check_GPU_space():\n",
        "  print(f\"Allocated space in GPU by PyTorch: {torch.cuda.memory_allocated() / 1e9} GB\")\n",
        "  print(f\"Reserved space in GPU by PyTorch: {torch.cuda.memory_reserved() / 1e9} GB\")\n",
        "\n",
        "check_GPU_space()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34cde133",
      "metadata": {
        "id": "34cde133"
      },
      "source": [
        "### Load training data\n",
        "  * `hendrycks/competition_math` is currently unavailable on HuggingFace.\n",
        "    * [Link to the dataset on HuggingFace](https://huggingface.co/datasets/hendrycks/competition_math)\n",
        "  * Let's use the copy `nlile/hendrycks-MATH-benchmark`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e61299e4",
      "metadata": {
        "id": "e61299e4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def collect_sft_data(\n",
        "  dataset_name = \"nlile/hendrycks-MATH-benchmark\",\n",
        "  config = None,\n",
        "  split = \"train\",\n",
        "  num_samples = None,\n",
        "  shuffle_seed = 42\n",
        "):\n",
        "    ds = load_dataset(dataset_name, config, split=split)\n",
        "    ds = ds.shuffle(seed=shuffle_seed)\n",
        "\n",
        "    if not num_samples: # use all\n",
        "      num_samples = len(ds)\n",
        "    elif isinstance(num_samples, int): # count mode\n",
        "      assert 0 <= num_samples\n",
        "      num_samples = min(len(ds), num_samples)\n",
        "      ds = ds.select(range(num_samples))\n",
        "    else: # fraction mode\n",
        "      assert isinstance(num_samples, float)\n",
        "      assert 0 <= num_samples <= 1.0\n",
        "      num_samples = int(num_samples * len(ds))\n",
        "      ds = ds.select(range(num_samples))\n",
        "\n",
        "    print(f\"{num_samples} samples of data are loaded.\")\n",
        "\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b58ff09b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "989f1ac9334d40548b8b32d277819a18",
            "6d032026677f4706a7b3c8e89b1471c3",
            "95333a9b7fe343c7a5f1f9281662f98e",
            "271f37b46d18434d99574d676a9444a1",
            "c6a191ebded4498ca04092c68df6967a",
            "f2e4a87a7a664b91a501d2df40f9318f",
            "a657efe4ab434cc7b57b77006242bee7",
            "34bc65b0b31045a79572021b0a92a299",
            "3f989fa268dc4fbf9c8ab8a73f8a0449",
            "ff0a77eecb184f3485932bc67e7baade",
            "e29f598aca644d4d804362e081724f38",
            "a623534c1c474d578db0bb2e3bf32302",
            "e6b0d969a49a4a0b9b1c21cbf5e2612d",
            "c68d47b551cf42fa9d85bc3c2a319edd",
            "312eacd1c4574c7b94c7ad454c6340d0",
            "6291ac7a507348158b5b66a7e05845f6",
            "510080016ea64ca0a5e4e281a8fc0702",
            "710899096f4c49678f746f8f3d7d1c93",
            "9ab9a8ca26a648efaa69c792e46da847",
            "b48f5d1a127f451789d26ce35be2cff9",
            "306b8fbe89a74a8686bb9a06759b5467",
            "2d926b8a91a94747969aa128524c4d2a",
            "9b6f32b9d39847ecac7270bda7843c42",
            "301f0e6abcba4b30a859e912eec3bec0",
            "5b08a5ef671c47c08f91976acbd16a83",
            "91455b3ed4b147998ce39782cc161fdf",
            "750ebe3c24d844b2a78d7308a5dca97f",
            "108aec7513904da0aa8c51e75457735c",
            "59e90796db0845258e59e2967034d306",
            "8b99814173d844c291167eaa865128c8",
            "443d2fe21ba44a6caf7374a610b170c0",
            "1619af3db00d4b7f831d71d1df62b1a6",
            "9982bbd0979a41bb9bb19f183d05d3bb",
            "a5ed3b05578e41aba8163ed4ae530045",
            "931beeff16c04ee0ae85ffdbd5a27f72",
            "ecb88a8215bc447dbc7f4dd2319ceb4e",
            "f42f38d6aa3b4d3388a4dee4a55a6765",
            "28fcbd30476847978967dc284361f1fd",
            "8bbdb67a395d468eb6f060897ce6c982",
            "f257237194704b64ac8112890ebb1b31",
            "41f9d6b70a0b4f8fbd56f0895a9c7d9a",
            "6e5dd2a370944577895c0ef2f692292f",
            "705d0334383b4e9a853d22666b279c51",
            "2821cf45a6234715a6d73e8cbddab69c",
            "b7f990945e024cf7a34ea32cf6cec3be",
            "5d07f87a52454e23966195f5bfb5c0b3",
            "37e88981873c40fb8409721f6849e6cc",
            "97b4092217c0492494923569b990dbab",
            "57adb9d322b74e3687925a8155ddac41",
            "3ba50f8746ba48de805d61236a7db020",
            "93f5bb6da3d249279b16ef13b0ba3d11",
            "c571fe02efdb4d9ead7a0ca3cb5b5450",
            "321fbf0f5ae545bf9eb8a8ab76bcdfc0",
            "37a706f1e0d04444a7d0c8de11c0ced8",
            "a3b50071c21e454c81d84b2c3abb986f"
          ]
        },
        "id": "b58ff09b",
        "outputId": "a727d43f-6d07-49ff-a48d-f12370d0499b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "989f1ac9334d40548b8b32d277819a18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/2.57k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a623534c1c474d578db0bb2e3bf32302",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/5.12M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b6f32b9d39847ecac7270bda7843c42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/210k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5ed3b05578e41aba8163ed4ae530045",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/12000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7f990945e024cf7a34ea32cf6cec3be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100 samples of data are loaded.\n"
          ]
        }
      ],
      "source": [
        "train_dataset = collect_sft_data(num_samples=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8aaea59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8aaea59",
        "outputId": "21d0dcb5-5d1e-43b2-be8e-28162d16c6ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['problem', 'solution', 'answer', 'subject', 'level', 'unique_id'],\n",
              "    num_rows: 100\n",
              "})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f016e189",
      "metadata": {
        "id": "f016e189"
      },
      "source": [
        "* The question-answer pairs are in strings. We need to tokenize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "949fce45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "949fce45",
        "outputId": "d4a288c2-bacf-4681-f9c1-af0d54068595"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An elephant and a lion are currently 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour.  How many minutes will it take for the lion to catch the elephant?\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset['problem'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19dafb12",
      "metadata": {
        "id": "19dafb12"
      },
      "outputs": [],
      "source": [
        "def preprocess_fn_batch(example, max_length=2048):\n",
        "    question = [f\"user: \\n{problem}\\nPlease reason step by step, and put your final answer within \\\\boxed.\\nassistant:\\n\" for problem in example['problem']]\n",
        "    solution = example[\"solution\"]\n",
        "    qa_pair = [question[i] + solution[i] for i in range(len(solution))]\n",
        "\n",
        "    qa_pair_tokenized = tokenizer(qa_pair,\n",
        "                     truncation=True,\n",
        "                     max_length=max_length,\n",
        "                     return_attention_mask=True)\n",
        "\n",
        "    input_ids = qa_pair_tokenized[\"input_ids\"]\n",
        "    question_ids = tokenizer(question,\n",
        "                  padding=False)[\"input_ids\"]\n",
        "\n",
        "    labels = []\n",
        "    for i, input in enumerate(input_ids):\n",
        "      label = input.copy()\n",
        "      question_len = len(question_ids[i])\n",
        "      # ignore the question tokens in loss calculation\n",
        "      label[:question_len] = [-100] * question_len\n",
        "      labels.append(label)\n",
        "\n",
        "    qa_pair_tokenized[\"labels\"] = labels\n",
        "    qa_pair_tokenized[\"answer\"] = example[\"answer\"]\n",
        "    return qa_pair_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "628cabb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d753f4d171ca4c3788e62954b383e3b3",
            "002a673f49874b79884353d40d340222",
            "2aabc06e38e148c89ce090765ee4707c",
            "bd506b099a234d598678a5e66f19a8ba",
            "76a592cfaaea44d29bddb3db55e952f1",
            "31dcc85065ae4db8b0089c1b948cec94",
            "ef8f16f9785d4ce38ddad4397822e758",
            "5839620b12bf4b5d91d6942bbe40cc8b",
            "0c8fde9c08af4356a335c84438646d61",
            "c0b8192c2180475a8809ad0cfccc7cdc",
            "6992b5ec704d4f7c8a05af868b38c05d"
          ]
        },
        "id": "628cabb4",
        "outputId": "97258619-6166-4d2f-8d7d-1383be1a7949"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d753f4d171ca4c3788e62954b383e3b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_dataset = train_dataset.map(\n",
        "    preprocess_fn_batch,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53dd3d1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53dd3d1f",
        "outputId": "0b4eb0be-5ebe-441e-e342-c1fa4a4ed1e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['answer', 'input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 100\n",
              "})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9468de2d",
      "metadata": {
        "id": "9468de2d"
      },
      "source": [
        "* Sanity check on the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f7c486c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f7c486c",
        "outputId": "df9178d2-213f-4b14-88f8-8ef5ef595cf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================\n",
            "Sample 0\n",
            "\n",
            "[answer] \n",
            "12\n",
            "\n",
            "[qa_pair_text] \n",
            "user: \n",
            "An elephant and a lion are currently 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour.  How many minutes will it take for the lion to catch the elephant?\n",
            "Please reason step by step, and put your final answer within \\boxed.\n",
            "assistant:\n",
            "Every hour, the lion runs 24 miles while the elephant runs 19.  Thus, the distance between the two animals closes at a rate of 5 miles every hour.  The lion catches the elephant after this distance has closed 1 mile, which takes $\\frac{1}{5}$ hours to do, or $\\frac{1}{5}\\cdot 60 = \\boxed{12}$ minutes.\n",
            "\n",
            "[labels_text] \n",
            "Every hour, the lion runs 24 miles while the elephant runs 19.  Thus, the distance between the two animals closes at a rate of 5 miles every hour.  The lion catches the elephant after this distance has closed 1 mile, which takes $\\frac{1}{5}$ hours to do, or $\\frac{1}{5}\\cdot 60 = \\boxed{12}$ minutes.\n",
            "\n",
            "===================\n",
            "Sample 1\n",
            "\n",
            "[answer] \n",
            "12\n",
            "\n",
            "[qa_pair_text] \n",
            "user: \n",
            "A triangle has vertices $A(-4, -1)$, $B (2, -1)$ and $C(1, 3)$. What is the area, in square units, of triangle $ABC$?\n",
            "Please reason step by step, and put your final answer within \\boxed.\n",
            "assistant:\n",
            "Let side $AB$ be the base; it has length $2+4=6$ since it is horizontal.  The altitude from $C$ to $AB$ is the length of the vertical distance from the line to $C$, which is $1+3=4$.  Thus the area is\n",
            "\n",
            "$$\\frac{6(4)}{2}=\\boxed{12}$$\n",
            "\n",
            "[labels_text] \n",
            "Let side $AB$ be the base; it has length $2+4=6$ since it is horizontal.  The altitude from $C$ to $AB$ is the length of the vertical distance from the line to $C$, which is $1+3=4$.  Thus the area is\n",
            "\n",
            "$$\\frac{6(4)}{2}=\\boxed{12}$$\n"
          ]
        }
      ],
      "source": [
        "check_sample = train_dataset.select(range(2))\n",
        "for i, example in enumerate(check_sample):\n",
        "    print(\"\\n===================\")\n",
        "    print(f\"Sample {i}\\n\")\n",
        "    answer = example[\"answer\"]\n",
        "    print(f\"[answer] \\n{answer}\\n\")\n",
        "    qa_pair_text = tokenizer.decode(example[\"input_ids\"], skip_special_tokens=True)\n",
        "    print(f\"[qa_pair_text] \\n{qa_pair_text}\\n\")\n",
        "    labels_ids = [t for t in example[\"labels\"] if t != -100]\n",
        "    labels_text = tokenizer.decode(labels_ids, skip_special_tokens=True)\n",
        "    print(f\"[labels_text] \\n{labels_text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60c5b0f7",
      "metadata": {
        "id": "60c5b0f7"
      },
      "source": [
        "### Load validation dataset\n",
        "* Evaluating with 500 samples can take a lot of time.\n",
        "* In addition, the CPU is also limited (only 12GB is available on Colab).\n",
        "* For this toy example, let's try 3 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ccca292",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "47042052568b46c3a43703f06b949eab",
            "f63dd6ae561447668e47108f1c9b2447",
            "5610217f19aa42e49a647198e6ee7b0f",
            "29a52562b39a4944a74d71ad2b5c15ab",
            "b7d8993e735240a28397dc864381eca2",
            "90a385a3f89e432284cb93696561d179",
            "e60a40017c534b1ca6fa43945c52f855",
            "98361e8caf22489dbabaa89e0fb621c9",
            "a97cf4fd50144120b4060314cabf7e1a",
            "b2551ee51dd14decb99f8e170950a6bd",
            "c77fe808c29c40aaa63bfe129f733613",
            "77148e662a3e48beaef15206a3ea8b02",
            "67575a4a712f4bf997bb3405dd106c85",
            "eead6823f9134ef4be77ade9df9a57f7",
            "392d7d5f328348119bd38e48740467fe",
            "3858a5e0d1fb4b808f2903a593f5e82c",
            "54bcb26e53044f2b9a13e04adeae857e",
            "2a9c87567e1d47ee8425ade0bf98b972",
            "b85ee7bfcb014543aea5f171fa1616bb",
            "56db0aa4967d4a4684fc952c7fb1d84f",
            "d2aee7c3607c4b24a762ae03cb13ae57",
            "c3519737bf4e4c3d956f178996f0c542",
            "50c109cb29dc49a289da9467fc1444fd",
            "945ff88ebdad4bf08b3f298e2d3ff8c5",
            "522dc73811134f9d9f1099cc1ef7cba8",
            "503b955cebd94f6abb3c660a3c6dc12f",
            "270e103ced17406b8ba9662cd72e77ca",
            "0bdf8789ffe94571914c2d1e56cdca35",
            "d0b0e4729a414c3ca29ad2ce920e67ed",
            "b196c23cf75e4c14b722529b26dcbf82",
            "f0bcf04456464604b0d3b527e7a769b8",
            "31fd5582d7134caa801dc306fa52a903",
            "a0c86183c22540e09de9b98b8e798eaa"
          ]
        },
        "id": "9ccca292",
        "outputId": "5bcb82d0-a061-45f4-ebe9-d737184d8f26"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47042052568b46c3a43703f06b949eab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/412 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77148e662a3e48beaef15206a3ea8b02",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test.jsonl:   0%|          | 0.00/447k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50c109cb29dc49a289da9467fc1444fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 samples of data are loaded.\n"
          ]
        }
      ],
      "source": [
        "val_ds = collect_sft_data(dataset_name=\"HuggingFaceH4/MATH-500\", split=\"test\", num_samples=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07e279d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07e279d4",
        "outputId": "ab3b9068-708c-49f8-8438-76fb7462b89d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['problem', 'solution', 'answer', 'subject', 'level', 'unique_id'],\n",
              "    num_rows: 3\n",
              "})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05bd6237",
      "metadata": {
        "id": "05bd6237"
      },
      "outputs": [],
      "source": [
        "def preprocess_fn_val_batch(example, max_length=2048):\n",
        "    question = [f\"user: \\n{problem}\\nPlease reason step by step, and put your final answer within \\\\boxed.\\nassistant:\\n\" for problem in example['problem']]\n",
        "    solution = example[\"solution\"]\n",
        "\n",
        "    question_tokenized = tokenizer(question,\n",
        "                     truncation=True,\n",
        "                     max_length=max_length,\n",
        "                     return_attention_mask=True)\n",
        "\n",
        "    question_tokenized[\"labels\"] = question_tokenized['input_ids']\n",
        "    question_tokenized[\"answer\"] = example[\"answer\"]\n",
        "    return question_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5c79c3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f3170344fd7d4062b66f63e299c575c3",
            "7e80e1500b9348a495b0e6a167414f44",
            "76daa32a7bfd476ab20bf1fc1caa11db",
            "0300e01e57904b498c30f0ee6942448b",
            "0b154a9cb98a4a61a759491da2b91c64",
            "0a9af21349b0404cb588c54ba80dae74",
            "08fe6ea0515f4113b2b2c3f900a4b79b",
            "31a2cd447a214ac28c1b4e7d5edbafa5",
            "b5f21f8f0a1d47d8b74c51acb8cc2f55",
            "d91a44fcedf1442d99c8bfa45da2d0da",
            "8f02e11468b24338adb676477e24716b"
          ]
        },
        "id": "b5c79c3e",
        "outputId": "bdeaf96b-9633-4ff8-e94e-64fd982fff61"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3170344fd7d4062b66f63e299c575c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "val_ds = val_ds.map(\n",
        "    preprocess_fn_val_batch,\n",
        "    batched=True,\n",
        "    remove_columns=val_ds.column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ed01b8d",
      "metadata": {
        "id": "5ed01b8d"
      },
      "source": [
        "* We need a special metric for MATH to handle Math equivalences of formula.\n",
        "  * It is implemented in HuggingFace: `evaluate-metric/competition_math`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9573b75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8edd2b4f36c84b10a76cd19f76ce3313",
            "408722e3e7e9421b8549f88f712dd5c5",
            "1f6b0ad3eeb8450986a900b1bef41a7b",
            "2bd66b72b7a54e5a889ff9bda7eb702a",
            "b5f7670eb9604b27a36178062af680eb",
            "1762c5cf5ff34619a93596959f66a4ac",
            "d9c4c112775a46c986e679a31cd6b485",
            "aabd6f8597454de1bb6159273669b568",
            "d01780f74f5041da8fae91b10c9b13f5",
            "fd267ba7a51b4e758c4ed3af5b6e6cfb",
            "bb138c4d463047f9a16027c61bf2b7a3"
          ]
        },
        "id": "a9573b75",
        "outputId": "099c2540-a2c9-4cbb-ca17-b13a9dc622ca"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8edd2b4f36c84b10a76cd19f76ce3313",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/3.24k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from evaluate import load\n",
        "math_metric = load(\"competition_math\")\n",
        "\n",
        "def math_acc(eval_pred):\n",
        "    logits, _ = eval_pred\n",
        "    pred_ids = np.argmax(logits, axis=-1)\n",
        "    decoded_preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    references = val_ds[\"answer\"]\n",
        "    results = math_metric.compute(predictions=decoded_preds,references=references)\n",
        "    return {\"math_acc\": results[\"accuracy\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b25077d",
      "metadata": {
        "id": "2b25077d"
      },
      "source": [
        "### Set up ZeRO configurations and understanding the DeepSpeed config\n",
        "* Since we are only using 1 GPU, we are not using data/tensor parallelization.\n",
        "* However, we can still use ZeRO for CPU Swapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daaf54bb",
      "metadata": {
        "id": "daaf54bb"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c2a1dd3",
      "metadata": {
        "id": "3c2a1dd3"
      },
      "source": [
        "*   `world_size`: number of devices\n",
        "*   `train_batch_size`: set to `auto` because we will set the batch size later.\n",
        "*   `zero_optimization`:\n",
        "    *   `stage`:\n",
        "        *   The ZeRO stage 1/2/3 we are using.\n",
        "        *   We are using ZeRO stage 3 since we are using CPU swapping for model weights.\n",
        "    *   `overlap_comm`:\n",
        "        *   Overlap backprop computation and communication.\n",
        "        *   reduce-scatter gradients as soon as it's ready.\n",
        "        *   Instead of waiting for the entire gradients to be ready.\n",
        "    *   `contiguous_gradients`:\n",
        "        *   Reduces fragmentations by storing the scattered gradients in a continuous buffer.\n",
        "    *   `reduce_bucket_size`:\n",
        "        *   Buffer for reduce-scatter operations.\n",
        "        *   On a single device, it is used for collect gradients and update weights.\n",
        "    *   `allgather_bucket_size`:\n",
        "        *   Buffer for all-gather operations.\n",
        "        *   On a single device, it is used for CPU swapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae310cb",
      "metadata": {
        "id": "fae310cb"
      },
      "outputs": [],
      "source": [
        "ds_config = {\n",
        "    \"world_size\": 1,\n",
        "    \"train_batch_size\": 'auto',\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
        "        \"offload_param\":   {\"device\": \"cpu\", \"pin_memory\": True},\n",
        "        \"overlap_comm\": False,\n",
        "        \"contiguous_gradients\": True,\n",
        "        \"reduce_bucket_size\": 2e8,\n",
        "        \"allgather_bucket_size\": 2e8\n",
        "    },\n",
        "    \"activation_checkpointing\": {\n",
        "        \"partition_activations\": False,\n",
        "        \"cpu_checkpointing\": True,\n",
        "        \"contiguous_memory_optimization\": True\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "201186f8",
      "metadata": {
        "id": "201186f8"
      },
      "source": [
        "*  \"offload_optimizer\":\n",
        "  *  Keep the Adam optimizer states on CPU.\n",
        "*  \"offload_param\":\n",
        "  *  Keep the model parameters on CPU.\n",
        "  *  Put them on GPU only for forward/backward passes.\n",
        "*  \"activation_checkpointing\":\n",
        "  *  Only saves some activation checkpoints on GPU.\n",
        "  *  Recompute the others during backprop.\n",
        "  *  \"cpu_checkpointing\": the saved checkpoints are saved on CPU instead.\n",
        "  *  \"partition_activations\":\n",
        "    *   Splits the activation checkpoints across ranks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "667a211d",
      "metadata": {
        "id": "667a211d"
      },
      "outputs": [],
      "source": [
        "with open(\"./ds_config.json\", \"w\") as f:\n",
        "    json.dump(ds_config, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "136151f0",
      "metadata": {
        "id": "136151f0"
      },
      "source": [
        "### Set up LoRA configurations\n",
        "* Check the matrices we are about to apply LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa0a42b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaa0a42b",
        "outputId": "3c2c8754-50db-4cf3-d9c7-6b3a15d29f35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 1536)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear8bitLt(in_features=1536, out_features=1536, bias=True)\n",
              "          (k_proj): Linear8bitLt(in_features=1536, out_features=256, bias=True)\n",
              "          (v_proj): Linear8bitLt(in_features=1536, out_features=256, bias=True)\n",
              "          (o_proj): Linear8bitLt(in_features=1536, out_features=1536, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear8bitLt(in_features=1536, out_features=8960, bias=False)\n",
              "          (up_proj): Linear8bitLt(in_features=1536, out_features=8960, bias=False)\n",
              "          (down_proj): Linear8bitLt(in_features=8960, out_features=1536, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dce8bceb",
      "metadata": {
        "id": "dce8bceb"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.gradient_checkpointing_enable()\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16, # rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bce69c8",
      "metadata": {
        "id": "1bce69c8"
      },
      "source": [
        "### Set up SFT configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1907e953",
      "metadata": {
        "id": "1907e953"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"MAX_JOBS\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1679b496",
      "metadata": {
        "id": "1679b496"
      },
      "source": [
        "* batch_size_per_device = per_device_train_batch_size * gradient_accumulation_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d33594b7",
      "metadata": {
        "id": "d33594b7"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    pad_to_multiple_of=8,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf2ab0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cf2ab0d",
        "outputId": "9d5b09c9-6947-4ff3-b30c-54704b7cfb49"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./DeepSeek-R1-Distill-Qwen-1.5B-MATH-SFT\",\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    logging_steps=50,\n",
        "    save_steps=50,\n",
        "    save_total_limit=1,\n",
        "    deepspeed=\"ds_config.json\",\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    # Validation\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=10,\n",
        "    per_device_eval_batch_size=1,\n",
        "    eval_accumulation_steps=1,\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"math_acc\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee889ee8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee889ee8",
        "outputId": "c13f49c7-3bc8-4b2d-80b9-b0117ff95273"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-33-6e3eba37e13c>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=math_acc\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2235e43",
      "metadata": {
        "id": "a2235e43"
      },
      "source": [
        "### Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53952f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "f53952f8",
        "outputId": "07272eb5-0cd3-4078-a157-4078e8b4dd67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installed CUDA version 12.5 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py311_cu124/cpu_adam...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/cpu_adam/build.ninja...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module cpu_adam...\n",
            "Using envvar MAX_JOBS (1) as the number of workers...\n",
            "Loading extension module cpu_adam...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to load cpu_adam op: 82.1094172000885 seconds\n",
            "Parameter Offload: Total persistent parameters: 2323968 in 253 params\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 02:21, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Math Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.640888</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.558287</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.414763</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.199883</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.914000</td>\n",
              "      <td>2.097744</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=50, training_loss=0.913958740234375, metrics={'train_runtime': 144.293, 'train_samples_per_second': 0.693, 'train_steps_per_second': 0.347, 'total_flos': 470966075392.0, 'train_loss': 0.913958740234375, 'epoch': 1.0})"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2220c117",
      "metadata": {
        "id": "2220c117"
      },
      "source": [
        "*   Peak GPU usage (achieved at training): 12.8GB\n",
        "*   Peak CPU usage (achieved at evaluating): 11.4GB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9ff0f6f",
      "metadata": {
        "id": "f9ff0f6f"
      },
      "source": [
        "### Materialize the adapter weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70ec2d9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70ec2d9f",
        "outputId": "a7b57bcf-8791-4dc3-b13d-56378cadff0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/DeepSeek-R1-Distill-Qwen-1.5B-MATH-SFT/checkpoint-50\n"
          ]
        }
      ],
      "source": [
        "cd DeepSeek-R1-Distill-Qwen-1.5B-MATH-SFT/checkpoint-50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d66af34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d66af34",
        "outputId": "24279b98-0047-4a3b-fe0a-5e270160d713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-11 23:45:32,129] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2025-05-11 23:45:43.012247: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747007143.225025    4078 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747007143.281748    4078 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Processing zero checkpoint './global_step50'\n",
            "Loading checkpoint shards: 100% 1/1 [00:00<00:00, 122.28it/s]\n",
            "Detected checkpoint of type zero stage ZeroStageEnum.weights, world_size: 1\n",
            "Parsing checkpoint created by deepspeed==0.16.7\n",
            "Gathering sharded weights: 100% 112/112 [00:00<00:00, 380066.38it/s]\n",
            "Reconstructed Trainable fp32 state dict with 112 params 2179072 elements\n",
            "Saving checkpoint shards: 100% 1/1 [00:00<00:00,  2.58it/s]\n"
          ]
        }
      ],
      "source": [
        "!python zero_to_fp32.py . DeepSeek-R1-Distill-Qwen-1.5B-MATH-SFT"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}